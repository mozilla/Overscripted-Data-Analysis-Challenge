It was natural for me, someone passionate about marketing and 
particularly web ad targeting, to choose a project that answers the 
most important question in ad based web development: _Is what we do now
sustainable?_

I am passionate about contributing to a project which seeks to uncover
the injustices of current web environment, where the financial 
underpinnings are the only driver for decision making. As someone new to 
the web tracking, I chose to contribute to the Literature Review. I 
believe it is of fundamental 
importance to understand the different ways web-tracking occurs on 
websites in order to narrow the scope of the research and target the 
most important. I believe contributing to this project will help us to 
further refine data collection, data munging and analysis.

I have used data derived from the November 2017 SRG Mozilla Web Crawl 
dataset and compiled in a CSV file by @birdsarah as well as different
literature to address the following questions about each web tracking
mechanism:
1. What is it/how does it track the information?
2. How can it be detected?
3. Can it be found within the dataset?
4. Is our data sufficient to answer the question 3?
5. How can we supplement the missing information?

After having reviewed the information in the literature provided, I 
faced a challenge of synthesizing the conceptrs with the dataset. 
I was directed by Sarah to the [article](https://medium.com/firefox-context-graph/overscripted-digging-into-javascript-execution-at-scale-2ed508f21862) 
on the dataset information and the work done so far. Re-referencing 
the information has clarified some fallacies in my logic and improved
my overall understanding of the dataset.

Looing forward to more feedback.

